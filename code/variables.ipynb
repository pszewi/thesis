{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc75f8e5",
   "metadata": {},
   "source": [
    "# Variable construction \n",
    "\n",
    "In this notebook I construct my variables for the final dataset. There are some notes about the taken choices made along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77006b17",
   "metadata": {},
   "source": [
    "#### Data Processing\n",
    "\n",
    "First reading in pre-processed data and loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce218eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from funcs import ExtractNameYear, ExtractFileName, ExtractAllText, ComputeGreenInd, TransformReturns, TransformIndices, MakeReturnsInd, MakeReturns, AbnormalReturns\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c2439",
   "metadata": {},
   "source": [
    "#### Loading the NLP models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6ede7",
   "metadata": {},
   "source": [
    "NLP model and greenwashing indicator (but first checking if I can use my GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14342c12",
   "metadata": {},
   "source": [
    "Classifying paragraphs into climate related/not related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a46713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model stuff\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e0520",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_class = \"climatebert/distilroberta-base-climate-detector\"\n",
    "\n",
    "# If you want to use your own data, simply load them as ðŸ¤— Datasets dataset, see https://huggingface.co/docs/datasets/loading\n",
    "\n",
    "model_class = AutoModelForSequenceClassification.from_pretrained(model_name_class)\n",
    "tokenizer_class = AutoTokenizer.from_pretrained(model_name_class, max_len=512)\n",
    "\n",
    "pipe_class = pipeline(\"text-classification\", model=model_class, tokenizer=tokenizer_class, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701bc6f9",
   "metadata": {},
   "source": [
    "*CLIMATEBERT* for checking the specificity of climate-related paragraphs **TESTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f081ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_spec = \"climatebert/distilroberta-base-climate-specificity\"\n",
    "\n",
    "model_spec = AutoModelForSequenceClassification.from_pretrained(model_name_spec)\n",
    "tokenizer_spec = AutoTokenizer.from_pretrained(model_name_spec, max_len=512)\n",
    "\n",
    "pipe_spec = pipeline(\"text-classification\", model=model_spec, tokenizer=tokenizer_spec, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde9a0d",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data on greenwashing companies\n",
    "data_greenwashing = pd.read_excel('../data/LSEG data/matched_final.xlsx', sheet_name='companies')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly returns\n",
    "# data_returns_m = pd.read_excel('../data/LSEG data/matched_final.xlsx', sheet_name='indicators_1')\n",
    "# data_indices_m = pd.read_excel('../data/LSEG data/matched_final.xlsx', sheet_name='INDICES_monthly', skiprows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly returns\n",
    "# data_returns_w = pd.read_excel('../data/LSEG data/matched_final.xlsx', sheet_name='indicators_2', skiprows=3)\n",
    "# data_indices_w = pd.read_excel('../data/LSEG data/matched_final.xlsx', sheet_name='INDICES_weekly', skiprows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad9866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily returns\n",
    "data_returns_d = pd.read_excel('../data/LSEG data/matched_final.xlsx', sheet_name='indicators_3', skiprows=3)\n",
    "data_indices_d = pd.read_excel('../data/LSEG data/matched_final.xlsx', sheet_name='INDICES_daily', skiprows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c548754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales\n",
    "data_sales = pd.read_excel('../data/LSEG data/matched_final.xlsx', sheet_name='indicators_4.1', skiprows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing returns and merging to greenwashing\n",
    "\n",
    "\n",
    "# data_returns_m = TransformReturns(data_returns_m, data_greenwashing, old=True)\n",
    "# data_returns_w = TransformReturns(data_returns_w, data_greenwashing)\n",
    "data_returns_d = TransformReturns(data_returns_d, data_greenwashing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_indices_m = TransformIndices(data_indices_m)\n",
    "# data_indices_w = TransformIndices(data_indices_w, weekly=True)\n",
    "data_indices_d = TransformIndices(data_indices_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d43e0b",
   "metadata": {},
   "source": [
    "#### Transforming the sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f482eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sales.loc[:, \"VARIABLE\"] = np.where(data_sales.loc[:,\"Name\"].str.contains(\"- QUARTER 1\"), 'Q1', pd.NA)\n",
    "data_sales.loc[:, \"VARIABLE\"] = np.where(data_sales.loc[:,\"Name\"].str.contains(\"- QUARTER 2\"), 'Q2', data_sales.loc[:, \"VARIABLE\"])\n",
    "data_sales.loc[:, \"VARIABLE\"] = np.where(data_sales.loc[:,\"Name\"].str.contains(\"- QUARTER 3\"), 'Q3', data_sales.loc[:, \"VARIABLE\"])\n",
    "data_sales.loc[:, \"VARIABLE\"] = np.where(data_sales.loc[:,\"Name\"].str.contains(\"- QUARTER 4\"), 'Q4', data_sales.loc[:, \"VARIABLE\"])\n",
    "\n",
    "\n",
    "data_sales.loc[:,\"NAME\"] = data_sales.loc[:,\"Name\"].str.replace(r\"- INTERIM SALES - QUARTER \\d{1}\", \"\", regex=True)\n",
    "data_sales.dropna(subset=[\"VARIABLE\"], inplace=True)\n",
    "data_sales[[\"TYPE\", \"VAR_CODE\"]] = data_sales[\"Code\"].str.split(\"(\", n=1, expand=True)\n",
    "data_sales.drop(columns=['Name', 'Code', 'VAR_CODE'], inplace=True)\n",
    "\n",
    "\n",
    "data_sales = pd.melt(data_sales, id_vars=['NAME', \"TYPE\", \"VARIABLE\"]).rename(columns={'variable':'DATE', \n",
    "                                                                         'value':'SALES'})\n",
    "\n",
    "data_sales[\"SALES\"] = data_sales[\"SALES\"].astype(float)\n",
    "data_sales[[\"QUARTER\", \"YEAR\"]] = data_sales[\"DATE\"].str.split(\" \", n=1, expand=True)\n",
    "data_sales[\"DATE\"] = data_sales[\"YEAR\"] + \" \" + data_sales[\"QUARTER\"]\n",
    "\n",
    "data_sales=data_sales.loc[:, ['NAME', 'TYPE','DATE','YEAR','QUARTER','SALES']].sort_values([\"NAME\", 'YEAR',\"QUARTER\"])\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "# making sure that all firms have data starting from the same date\n",
    "full_index = pd.DataFrame(product(data_sales[\"NAME\"].unique(), data_sales[\"DATE\"].unique()))\n",
    "full_index.columns = [\"NAME\", \"DATE\"]\n",
    "\n",
    "data_sales.dropna(subset=[\"SALES\"], inplace=True)\n",
    "\n",
    "data_sales = full_index.merge(data_sales, how=\"left\", on=['NAME', 'DATE'])\n",
    "\n",
    "data_sales = data_sales.groupby(\"NAME\").filter(lambda x: ~x[\"SALES\"].isna().any())\n",
    "data_sales = data_sales.groupby(\"NAME\").filter(lambda x: (x[\"SALES\"]>0).all())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e18b3f",
   "metadata": {},
   "source": [
    "**Calculating the log differences for sales** \\\n",
    "\\\n",
    "Here I will surely have some missing data, and so I am trying to make the log_change_sales such that there's no loss of variables. **NOTE** I will need to adjust for seasonality in sales! \\\n",
    "\\\n",
    "I WILL DEFINITELY NEED TALK ABOUT THE EXCLUSION OF FIRMS IN MY SAMPLES - AFTER ALL, A LOT OF FIRMS DROPS OUT!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sales[\"LOG_SALES\"] = np.log((data_sales[\"SALES\"]))\n",
    "data_sales[\"LOG_SALES_DIFF\"] = data_sales.groupby(by='NAME')['LOG_SALES'].diff()\n",
    "data_sales.to_csv(\"../data/output/data_sales.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a53257c",
   "metadata": {},
   "source": [
    "#### Calculating abnormal returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea8f1a",
   "metadata": {},
   "source": [
    "Here I calculate abnormal returns and cumulative abnormal returns. In order to do that, I benchmark the realized returns of each of the companies against their expected return. The market model is estimated in order to assess the expected return. The market for each security is chosen with respect to the primary market of operations of each company (S&P for the USA/Canada, MSCI EUROPE for Europe, ... **Work-in-progress, need to get indices for more geographical regions**). Finally, I chose to use the price return on equity only (instead of total return including re-invested dividends - they don't really matter over such a short period anyway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8378a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see the market from which most of the companies come from (\"Country of Domicile\")\n",
    "print(data_greenwashing['CTRY_OF_DOM_NAME'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "# data_returns_m = MakeReturns(data_returns_m)\n",
    "# data_returns_w = MakeReturns(data_returns_w)\n",
    "data_returns_d = MakeReturns(data_returns_d)\n",
    "\n",
    "\n",
    "# data_indices_m = MakeReturnsInd(data_indices_m)\n",
    "# data_indices_w = MakeReturnsInd(data_indices_w)\n",
    "data_indices_d = MakeReturnsInd(data_indices_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a216200",
   "metadata": {},
   "source": [
    "Checking for duplicated data in the companies. It seems that some have been duplicated but mostly due to having a subsidiary in the same name/having multiple pages etc. \n",
    "Some of them are also the wrong company. There's little enough problems though, that it seems like it could be dropped (16 companies like that) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_returns_m[['NAME', 'DATE']].value_counts())\n",
    "# print(data_returns_w[['NAME', 'DATE']].value_counts())\n",
    "# print(data_returns_d[['NAME', 'DATE']].value_counts())\n",
    "\n",
    "# data_returns_m.drop_duplicates(subset=['NAME', 'DATE'], inplace=True)\n",
    "# data_returns_w.drop_duplicates(subset=['NAME', 'DATE'], inplace=True)\n",
    "data_returns_d.drop_duplicates(subset=['NAME', 'DATE'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_dict = {\n",
    "                # S&P 500\n",
    "                \"UNITED STATES\":\"S&P 500 COMPOSITE\",\n",
    "                \"CANADA\":\"S&P 500 COMPOSITE\",\n",
    "                \"BERMUDA\":\"S&P 500 COMPOSITE\",\n",
    "                \"CAYMAN ISLANDS\":\"S&P 500 COMPOSITE\",\n",
    "                \n",
    "                # MSCI EM MARKETS AMERICA\n",
    "                \"MEXICO\":\"MSCI EM LATIN AMERICA U$\",\n",
    "                \"PUERTO RICO\":\"MSCI EM LATIN AMERICA U$\",\n",
    "                \"COSTA RICA\":\"MSCI EM LATIN AMERICA U$\",\n",
    "                \"BARBADOS\":\"MSCI EM LATIN AMERICA U$\",\n",
    "                \"PANAMA\":\"MSCI EM LATIN AMERICA U$\",\n",
    "                \"COLOMBIA\":\"MSCI EM LATIN AMERICA U$\",\n",
    "                \"BRAZIL\":\"MSCI EM LATIN AMERICA U$\",\n",
    "                \"CHILE\":\"MSCI EM LATIN AMERICA U$\",\n",
    "                \"PERU\":\"MSCI EM LATIN AMERICA U$\",\n",
    "                \"URUGUAY\":\"MSCI EM LATIN AMERICA U$\",\n",
    "                \"ARGENTINA\":\"MSCI EM LATIN AMERICA U$\",\n",
    "                \n",
    "                # MSCI europe\n",
    "                \"UNITED KINGDOM\":\"MSCI EUROPE U$\",\n",
    "                \"IRELAND\":\"MSCI EUROPE U$\",\n",
    "                \"SWITZERLAND\":\"MSCI EUROPE U$\",\n",
    "                \"NETHERLANDS\":\"MSCI EUROPE U$\",\n",
    "                \"GREECE\":\"MSCI EUROPE U$\",\n",
    "                \"GERMANY\":\"MSCI EUROPE U$\",\n",
    "                \"BELGIUM\":\"MSCI EUROPE U$\",\n",
    "                \"DENMARK\":\"MSCI EUROPE U$\",\n",
    "                \"MONACO\":\"MSCI EUROPE U$\",\n",
    "                \"LUXEMBOURG\":\"MSCI EUROPE U$\",\n",
    "                \"FRANCE\":\"MSCI EUROPE U$\",\n",
    "                \"SWEDEN\":\"MSCI EUROPE U$\",\n",
    "                \"ISLE OF MAN\":\"MSCI EUROPE U$\",\n",
    "                \"SPAIN\":\"MSCI EUROPE U$\",\n",
    "                \"FINLAND\":\"MSCI EUROPE U$\",\n",
    "                \"ROMANIA\":\"MSCI EUROPE U$\",\n",
    "                \"ITALY\":\"MSCI EUROPE U$\",\n",
    "                \"AUSTRIA\":\"MSCI EUROPE U$\",\n",
    "                \"JERSEY\":\"MSCI EUROPE U$\",\n",
    "                \"GUERNSEY\":\"MSCI EUROPE U$\",\n",
    "                \"TURKEY\":\"MSCI EUROPE U$\",\n",
    "                \n",
    "                \n",
    "                # msci pacific\n",
    "                \"HONG KONG\":\"MSCI PACIFIC U$\",\n",
    "                \"SINGAPORE\":\"MSCI PACIFIC U$\",\n",
    "                \"JAPAN\":\"MSCI PACIFIC U$\",\n",
    "                \"AUSTRALIA\":\"MSCI PACIFIC U$\",\n",
    "                \"NEW ZEALAND\":\"MSCI PACIFIC U$\",\n",
    "                \"PAPUA NEW GUINEA\":\"MSCI PACIFIC U$\",\n",
    "                \n",
    "                # MSCI AC ASIA\n",
    "                \"CHINA\":\"MSCI AC ASIA U$\",\n",
    "                \"INDIA\":\"MSCI AC ASIA U$\",\n",
    "                \"SOUTH KOREA\":\"MSCI AC ASIA U$\",\n",
    "                \"TAIWAN\":\"MSCI AC ASIA U$\",\n",
    "                \"MONGOLIA\":\"MSCI AC ASIA U$\",\n",
    "                \"INDONESIA\":\"MSCI AC ASIA U$\",\n",
    "                \"PHILIPPINES\":\"MSCI AC ASIA U$\",\n",
    "                \n",
    "                # MSCI WORLD\n",
    "                \"ISRAEL\":\"MSCI WORLD U$\",\n",
    "                \"KAZAKHSTAN\":\"MSCI WORLD U$\",\n",
    "                \"UNITED ARAB EMIRATES\":\"MSCI WORLD U$\",\n",
    "                \"SOUTH AFRICA\":\"MSCI WORLD U$\",\n",
    "                \n",
    "                # missing values\n",
    "                np.nan:'NA'\n",
    "               \n",
    "    \n",
    "}\n",
    "\n",
    "exchange_mrkt_dict = {\n",
    "# US\n",
    "\"Nasdaq\":\"S&P 500 COMPOSITE\",\n",
    "\"Toronto SE\":\"S&P 500 COMPOSITE\",\n",
    "\"NYSE\":\"S&P 500 COMPOSITE\",\n",
    "\n",
    "# Europe\n",
    "\"London SE\":\"MSCI EUROPE U$\",       \n",
    "\"Euronext Amsterdam\":\"MSCI EUROPE U$\",\n",
    "\"NASDAQ Stockholm\":\"MSCI EUROPE U$\",\n",
    "\"Boerse Frankfurt\" :\"MSCI EUROPE U$\",\n",
    "\"Euronext Paris\":\"MSCI EUROPE U$\",\n",
    "\"Six Swiss Exchange\":\"MSCI EUROPE U$\",\n",
    "\"Euronext Brussels\" :\"MSCI EUROPE U$\",\n",
    "\"Borsa Italiana\":\"MSCI EUROPE U$\",\n",
    "\"Oslo Bors\":\"MSCI EUROPE U$\",\n",
    "\"Wiener Boerse AG\":\"MSCI EUROPE U$\",\n",
    "\"Athens SE\" :\"MSCI EUROPE U$\",\n",
    "\"NASDAQ Helsinki\":\"MSCI EUROPE U$\",\n",
    "\"NASDAQ Copenhagen\":\"MSCI EUROPE U$\",\n",
    "\"Boerse Hamburg\":\"MSCI EUROPE U$\",\n",
    "\"BME Exchange\":\"MSCI EUROPE U$\",\n",
    "\"Ljubljana SE\":\"MSCI EUROPE U$\",\n",
    "\n",
    "# Pacific\n",
    "\"Tokyo SE\":\"MSCI PACIFIC U$\",\n",
    "\"Hong Kong Exchange\":\"MSCI PACIFIC U$\",\n",
    "\"Singapore Exchange\":\"MSCI PACIFIC U$\",\n",
    "\"Australian SE\":\"MSCI PACIFIC U$\",\n",
    "\"New Zealand Exchange\":\"MSCI PACIFIC U$\",\n",
    "\n",
    "# Asia\n",
    "\"Korea Exchange\":\"MSCI AC ASIA U$\",    \n",
    "\"Taiwan SE\":\"MSCI AC ASIA U$\",\n",
    "\"National SE\":\"MSCI AC ASIA U$\",\n",
    "\n",
    "# latin america\n",
    "\"Santiago SE\":\"MSCI EM LATIN AMERICA U$\",\n",
    "\"Bolsa Mexicana\":\"MSCI EM LATIN AMERICA U$\",\n",
    "\n",
    "# Other\n",
    "\"Egyptian Exchange\":\"MSCI WORLD U$\",\n",
    "\"Johannesburg SE\":\"MSCI WORLD U$\",\n",
    "np.nan:'NA'\n",
    "}\n",
    "\n",
    "\n",
    "# data_abnormal_returns_m = AbnormalReturns(data_returns_m, data_indices_m,  market_dict, \"2014-01-01\", \"2018-06-01\", \"STOCK_LOG_RETURN\", \"INDEX_LOG_RETURN\")\n",
    "# data_abnormal_returns_w = AbnormalReturns(data_returns_w, data_indices_w,  market_dict, \"2016-01-01\", \"2019-07-01\", \"STOCK_LOG_RETURN\", \"INDEX_LOG_RETURN\")\n",
    "data_abnormal_returns_d = AbnormalReturns(data_returns_d, data_indices_d,  market_dict, exchange_mrkt_dict, \"2019-01-01\", \"2019-08-01\", \"STOCK_LOG_RETURN\", \"INDEX_LOG_RETURN\")\n",
    "\n",
    "\n",
    "# data_abnormal_returns_m.to_csv(\"../data/output/data_abnormal_returns_m.csv\", index=False)\n",
    "# data_abnormal_returns_w.to_csv(\"../data/output/data_abnormal_returns_w.csv\", index=False)\n",
    "data_abnormal_returns_d.to_csv(\"../data/output/data_abnormal_returns_d.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74218a7",
   "metadata": {},
   "source": [
    "### Transforming the greenwashing dataset\n",
    "Making one observation per sustainability report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /////////////////////////////////////\n",
    "#       expanding\n",
    "# /////////////////////////////////////\n",
    "\n",
    "data_greenwashing['REPORT_LISTS'] = data_greenwashing['REPORT_LISTS'].apply(eval)\n",
    "\n",
    "# Expand each list entry into its own row\n",
    "data_greenwashing = data_greenwashing.explode('REPORT_LISTS', ignore_index=True)\n",
    "\n",
    "data_greenwashing['YEAR'] = data_greenwashing['REPORT_LISTS'].str.extract(r'(\\d+)')\n",
    "data_greenwashing['YEAR'] = data_greenwashing['YEAR'].astype(float)\n",
    "\n",
    "data_greenwashing.to_csv(\"../data/output/company_characteristics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets that have a 2017 sustainability report\n",
    "\n",
    "greenwashing_2017 = data_greenwashing[data_greenwashing[\"YEAR\"]==2017]\n",
    "names_2017 = greenwashing_2017[\"NAME_SCRAPED\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d81b70",
   "metadata": {},
   "source": [
    "#### Greenwashing Indicator - **2017**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502f1de",
   "metadata": {},
   "source": [
    "Expanding the dataset to have one observation per sustainability report and calculating the number of reports per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greenwashing_2017.value_counts(\"CTRY_OF_DOM_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ff887",
   "metadata": {},
   "source": [
    "Converting the 2017 sustainability reports into lists of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "input_name = \"C:/Users/Jakub/OneDrive - Tilburg University/thesis data/responsibility reports\"\n",
    "\n",
    "filenames = ExtractFileName(series_names=names_2017, input_dir=input_name, year_str=\"2017\")\n",
    "\n",
    "names_2017_updated = names_2017.str.lower().str.replace(\" \", \"_\").str.replace(\"?\", \"\").str.replace(\"|\", \"\")\n",
    "names_2017_updated = names_2017_updated.loc[~(names_2017_updated.isin(filenames.keys()))]\n",
    "\n",
    "filenames_updated = ExtractNameYear(series_names=names_2017_updated, input_dir=input_name, nlp_model=nlp, year_str=\"2017\")\n",
    "\n",
    "filenames.update(filenames_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0b94a",
   "metadata": {},
   "source": [
    "Dumping all of the text from PDF files into their respective TXT files for later processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtractAllText(filenames, \"2017\", \"../data/text_processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf4740",
   "metadata": {},
   "source": [
    "Here I need to load all of the files in a loop, then categorize them, subset the categorized ones and within those classify some as specific and nonspecific and then compute the prop of non-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8edc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "greenwashing_ind = ComputeGreenInd(names_2017, \"../data/text_processing/2017\", pipe_class, pipe_spec)\n",
    "\n",
    "greenwashing_ind.to_csv(\"../data/text_processing/greenwashing_ind_2017.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46b012",
   "metadata": {},
   "source": [
    "#### Greenwashing Indicator - **2018**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b329c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets that have a 2017 sustainability report\n",
    "\n",
    "greenwashing_2018 = data_greenwashing[data_greenwashing[\"YEAR\"]==2018]\n",
    "names_2018 = greenwashing_2018[\"NAME_SCRAPED\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7b859",
   "metadata": {},
   "source": [
    "Expanding the dataset to have one observation per sustainability report and calculating the number of reports per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d43a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greenwashing_2018.value_counts(\"CTRY_OF_DOM_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8104ef",
   "metadata": {},
   "source": [
    "Converting the 2018 sustainability reports into lists of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ffcab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "input_name = \"C:/Users/Jakub/OneDrive - Tilburg University/thesis data/responsibility reports\"\n",
    "\n",
    "filenames = ExtractFileName(series_names=names_2018, input_dir=input_name, year_str=\"2018\")\n",
    "\n",
    "names_2018_updated = names_2018.str.lower().str.replace(\" \", \"_\").str.replace(\"?\", \"\").str.replace(\"|\", \"\")\n",
    "names_2018_updated = names_2018_updated.loc[~(names_2018_updated.isin(filenames.keys()))]\n",
    "\n",
    "filenames_updated = ExtractNameYear(series_names=names_2018_updated, input_dir=input_name, nlp_model=nlp, year_str=\"2018\")\n",
    "\n",
    "filenames.update(filenames_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba22e6d",
   "metadata": {},
   "source": [
    "Dumping all of the text from PDF files into their respective TXT files for later processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtractAllText(filenames, \"2018\", \"../data/text_processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4a1bb",
   "metadata": {},
   "source": [
    "Computing the greenwashing indicator and writing it to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "greenwashing_ind = ComputeGreenInd(names_2018, \"../data/text_processing/2018\", pipe_class, pipe_spec)\n",
    "\n",
    "greenwashing_ind.to_csv(\"../data/text_processing/greenwashing_ind_2018.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
