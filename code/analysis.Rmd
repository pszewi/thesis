---
title: "Data Analysis for Thesis"
author: "Jakub Przewoski"
date: "2025-04-28"
output: html_document
---

# Description
In this file I will be writing down the methodological notes for my thesis and I will also compute here my experiment. 



## Experiment-setup:
I am trying to investigate whether firms can utilize greenwashing as a profitable strategy. In order to do that I want to create a measure of greenwashing by following *(Lagasio, 2024)*. Furthermore, I will utilize a natural experiment (DiD) to search for causality.


## Experiment notes:
* Use a DiD with continuous treatment to see whether greenwashing more makes you profit more
* For applying treatment, use the level of greenwashing from 2017/2018
    * The justification is that I need to assume that the treatment level was constant which implies that I should have as little time as possible between the treatment assignment and my shock 
    * I have computed both the 2017 and the 2018 Greenwashing index.
* For the shock, use September 2019, as that's when there was a lot of buzz around another IPCC report and Fridays For Future movement
    * use cumulative abnormal returns as the outcome variable 
    * use sales as a secondary outcome variable

Some pictures showing google trends around specific queries around that time:

<div>
<img src="../images/ipcc_trends_2008_2021.png" width="600"/>
</div>


<div>
<img src="../images/climate_trends_2004_2022.png" width="600"/>
</div>


----

## Some links:
* [IPPC Wikipedia Page](https://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change)
* [6th IPCC Assessment](https://en.wikipedia.org/wiki/IPCC_Sixth_Assessment_Report)
* [IPCC Special Report from 2019](https://en.wikipedia.org/wiki/Special_Report_on_Climate_Change_and_Land)
* [Fridays For Future movement Wiki](https://en.wikipedia.org/wiki/Fridays_for_Future)
* [List of school climate strikes](https://en.wikipedia.org/wiki/List_of_school_climate_strikes)


----

# Code

## Loading libraries


```{r, libs, message = FALSE}
library(httpgd)
library(tidyverse)
library(data.table)
library(ggplot2)
library(fixest)
library(stargazer)
library(readxl)
library(stringr)
library(grid)
library(gridExtra)
library(zoo)
library(stargazer)
```

## User-defined functions

```{r, funcs}
# function for making my own event study plots
eve_by_t <- function(object, t_group, event_date, daily = FALSE, qtr = FALSE) {
    # x axis data
    x_axis_data <- unique(as.numeric(str_extract(rownames(object$coeftable), "(?<=TimeTo_T::).*")))
    dates <- event_date + x_axis_data # nolint: brace_linter.
    
    if (daily) {
        # proccess if data is daily with weekends included
        ref <- -1

    } else if (qtr) {
        # process if data is not daily and doesn't have gaps
        correct_time <- seq(min(x_axis_data), max(x_axis_data), length = (length(x_axis_data) + 1))
        ref <- correct_time[!(correct_time %in% x_axis_data)]
    } else {
        stop("Either daily of qtrly frequency must be chosen!")
    }


    # make values
    estimates <- c(as.numeric(object$coeftable[startsWith(rownames(object$coeftable), t_group), "Estimate"]))
    ci_h <- confint(object)[startsWith(rownames(object$coeftable), t_group), "97.5 %"]
    ci_l <- confint(object)[startsWith(rownames(object$coeftable), t_group), "2.5 %"]
    plot_df <- cbind(dates, estimates, ci_h, ci_l)
    plot_df <- rbind(plot_df, c((event_date + ref), 0, NA, NA))
    plot_df <- as.data.frame(plot_df) %>% arrange(dates)
    if (daily) {
        plot_df$dates <- as.Date(plot_df$dates)
    } else if (qtr) {
        plot_df$dates <- as.yearqtr(plot_df$dates)
        plot_df$dates <- as.character(plot_df$dates)
    }
    

    # make plot
    p <- ggplot(data = as.data.frame(plot_df)) +
        geom_hline(yintercept = 0, linetype = "dashed", color = "grey50") +
        # geom_vline(xintercept = ref, linetype = "dashed", color = "grey50") +
        geom_errorbar(aes(x = dates, ymin = ci_l, ymax = ci_h),
                      width = 0, color = "black") +
        geom_point(aes(x = dates, y = estimates),
                   size = 2, color = "black", shape = 19) +
        {if (daily) {
            scale_x_date(date_breaks = "3 days", date_labels = "%b %d")
        } else if (qtr) {
            # scale_x_discrete(breaks = dates)
        }} +
        theme_bw(base_size = 12) +
        theme(
            text = element_text(family = "Times"),
            axis.text = element_text(size = 13),
            axis.text.x = element_text(angle = 45, hjust = 1),
            axis.title.x = element_blank(),
            panel.grid.minor = element_blank(),
            panel.grid.major = element_line(color = "grey95"),
            axis.title = element_text(size = 16),
            plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), "cm")
        ) +
        labs(
            y = "Estimate \\ 95% CI"
        )

    p
}

get_acrt <- function(feols_obj){
coefs <- as.data.frame(feols_obj$coeftable)
coefs$doses <- unique(as.numeric(str_extract(rownames(feols_obj$coeftable), "(?<=T_cont::)\\d")))
coefs$ACRT <- NA
coefs$ACRT_se <- NA
coefs$ACRT_se <- NA
coefs$ACRT_t.val <- NA
coefs$ACRT_p.val <- NA
dof <- feols_obj$fixef_sizes[1] - 1

for(i in c(2:nrow(coefs))){
    est_diff <- coefs$Estimate[i] - coefs$Estimate[(i - 1)]
    vcov_mat <- vcov(feols_obj)
    varr_diff <- vcov_mat[i, i] + vcov_mat[(i - 1), (i - 1)] - (2 * vcov_mat[i, (i - 1)])
    coefs$ACRT[i] <- est_diff
    coefs$ACRT_se[i]  <- sqrt(varr_diff)
    coefs$ACRT_t.val[i] <- (coefs$ACRT[i] / coefs$ACRT_se[i])
    coefs$ACRT_p.val[i] <- 2 * pt(abs(coefs$ACRT_t.val[i]), df = dof, lower.tail = FALSE)

}

coefs %>% select(ACRT, ACRT_se, ACRT_t.val, ACRT_p.val) %>% rename('Std. Error' = ACRT_se, 't value' = ACRT_t.val, 'Pr(>|t|)' = ACRT_p.val)
}

```


## Trends in articles data

```{r, nyt_trends}
trends <- read_excel("data/nyt_query.xlsx")

trends <- trends %>%
    group_by(Query) %>%
    mutate(new_art = num - lag(num)) %>%
    ungroup() %>%
    mutate(Date = as.Date(trends$Date))  %>%
    filter(Date > as.Date("2010-01-01"))


# making shares as david requested
trends_total <- trends %>%
    filter(Query == "Total") %>%
    select(Year, new_art) %>%
    rename("new_art_total" = new_art)

trends <- trends %>%
    merge(trends_total, by = "Year", all.x = TRUE) %>%
    filter(Query != "Total") %>%
    mutate(num_share = (new_art / new_art_total))


ggplot(data = trends) +
    geom_line(aes(x = Year, y = num_share), linewidth = 0.7) +
    facet_wrap(~Query, scales = "free_y", ncol = 3) +
    scale_x_continuous(breaks = seq(2010, 2023, 2)) +
    scale_y_continuous(labels = scales::percent) +
    geom_vline(aes(xintercept = 2019),
               color = "red", linetype = "dashed", size = 0.7) +
    theme_bw(base_size = 12) +
    theme(
        text = element_text(family = "Times"),
        axis.text = element_text(size = 13),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_text(size = 16),
        axis.title.x = element_blank(),
        panel.grid.minor = element_blank(),
        panel.spacing = unit(1, "lines"),
        # legend.position = "none",
        strip.text = element_text(face = "bold", size = 12),
        strip.background = element_rect(fill = "white"),
        plot.title = element_text(size = 14, hjust = 0),
        plot.subtitle = element_text(size = 12, hjust = 0),
        plot.caption = element_text(size = 10, hjust = 0, face = "italic")
    ) +
    scale_color_grey(start = 0.2, end = 0.8) +
    labs(
        # title = "Growth in New York Times Coverage by Topic",
        # subtitle = "Number of new articles published per year (2010-2023)",
        x = "Year",
        y = "Share of New Articles",
        # caption = "Source: New York Times"
    )


ggsave("images/output/nyt_articles.pdf", height=5.5, width=11)
```

## Google trends data
```{r, google_trends}

fff_google <- read.csv("data/FFF_googletrends.csv", skip = 2,
                       colClasses = c("Date", "character"), 
                       col.names = c("week", "value")) %>%
    mutate(
        value = ifelse(value == "<1", 1, value),
        value = as.integer(value)
    )
# View(fff_google)

# Identify significant spikes in the data (top 5%)
fff_google <- fff_google %>%
    mutate(is_spike = value > quantile(value, 0.95, na.rm = TRUE))

# Create plot with academic styling
ggplot(data = fff_google) +
        geom_line(aes(x = week, y = value, group = 1), linewidth = 0.7, color = "black") +
        geom_point(data = subset(fff_google, is_spike), 
                             aes(x = week, y = value), 
                             color = "darkred", size = 2, shape = 19) +
        theme_classic(base_size = 12) +
        theme(
                text = element_text(family = "Times"),
                axis.text = element_text(size = 13),
                axis.text.x = element_text(angle = 45, hjust = 1),
                axis.title = element_text(size = 16),
                axis.title.x = element_blank(),
                panel.grid.major.y = element_line(color = "gray90", linewidth = 0.3),
                panel.grid.minor = element_blank(),
                plot.margin = unit(c(1, 1, 1, 1), "lines")
        ) +
        scale_x_date(date_breaks = "3 months", date_labels = "%b %Y") +
        labs(
                # x = "Time",
                y = "Relative search interest (%)"
        )

ggsave("images/output/fff_trend.pdf", height = 5, width=10)
```

## Loading Data for Analysis

First loading here data at the weekly frequency as it seems the most reasonable.

```{r,data, message = FALSE}
company_characteristics <- read_csv("data/output/company_characteristics.csv")
greenwashing_ind_2018  <- read_csv("data/text_processing/greenwashing_ind_2018.csv")


data_abnormal_returns_d <- read_csv("data/output/data_abnormal_returns_d.csv")
data_sales  <- read_csv("data/output/data_sales.csv")

```

Adjusting the universal data (needed for all regressions)

```{r, char_green_data}

# get rid of an observation with a missing indicator and faulty outliers
greenwashing_ind_2018 <- greenwashing_ind_2018 %>%
    filter(!(NAME %in% c("Genmab A/S", "Ashland Inc.", "Fletcher Building Limited", "Ethan Allen Interiors Inc.",
                         "Equifax Inc.", "Sagicor Financial Company Ltd.", "Korn/Ferry International",
                         "Spark NZ", "Covestro AG")),
           CLIMATE_REL > 2)  %>%
    mutate(GREEN_IND_2 = NON_SPEC / CLIMATE_REL,
        T_cont = case_when(
            GREEN_IND_2 < quantile(GREEN_IND_2, prob = c(0.25)) ~ 1,
            (GREEN_IND_2 >= quantile(GREEN_IND_2, prob = c(0.25))) &
                (GREEN_IND_2 < quantile(GREEN_IND_2, prob = c(0.5))) ~ 2,
            (GREEN_IND_2 >= quantile(GREEN_IND_2, prob = c(0.5))) &
                (GREEN_IND_2 < quantile(GREEN_IND_2, prob = c(0.75))) ~ 3,
            GREEN_IND_2 >= quantile(GREEN_IND_2, prob = c(0.75)) ~ 4
        ),
        T_ = ifelse(GREEN_IND > mean(GREEN_IND), 1, 0)
    )


company_characteristics  <- company_characteristics  %>% filter(YEAR == 2018)
company_characteristics_2018 <- merge(company_characteristics, greenwashing_ind_2018,
                                      by.x = "NAME_SCRAPED", by.y = "NAME") %>%
    select(c("TYPE", "NAME", "BOURSE_NAME", "ICB_INDUSTRY_NAME", "ICB_SECTOR_NAME", "CTRY_OF_DOM_NAME",
             "CTRY_OF_INC_NAME", "EMPLOYEES", "YEAR", "GREEN_IND_2", "T_", "T_cont")) %>%
    filter(!duplicated(NAME))

```


## Descriptive statistics

```{r, desc_stats, asis=TRUE}

ggplot(greenwashing_ind_2018) +
    geom_density(aes(x = GREEN_IND), color = "red") +
    geom_density(aes(x = GREEN_IND_2), color = "blue")





ggplot(data = company_characteristics_2018,
       #    aes(x = factor(ICB_INDUSTRY_NAME))) +
       aes(x = factor(ICB_INDUSTRY_NAME), fill = factor(T_))) +
    # geom_bar(fill   = "steelblue", color  = "black", width  = 0.7) +
    geom_bar(position = "fill", width = 0.7) +
    labs(
         x     = "Category",
         y     = "Count",
         title = "Distribution of Categories", 
         fill="Treatment") +
    theme_bw(base_size = 14) +
    theme(
          panel.grid.major    = element_blank(),
          panel.grid.minor    = element_blank(),
          axis.text.x         = element_text(angle = 45, hjust = 1),
          plot.title          = element_text(hjust = 0.5, face = "bold"),
          axis.title          = element_text(face = "bold"))



# here I made some tables for the latex document (descriptive statistics)
# library(gtsummary)

# tbl_ltx = company_characteristics_2018 %>%
#     select(CTRY_OF_DOM_NAME, T_cont) %>%
#     tbl_summary(
#                 by = T_cont,
#                 statistic = list(CTRY_OF_DOM_NAME ~ "{n} ({p}%)")) %>%
#     add_overall() %>%
#     modify_header(
#                   label     = "Number of ...",
#                   all_stat_cols() ~ "{level} (N = {n})") %>%
#     modify_table_body(~.x %>% relocate(stat_0, stat_2, stat_1, .after = label))


# tbl_ltx %>%
#   as_kable(
#     format    = "latex",
#     booktabs  = TRUE
#   ) 

# # number of reports per year
# tbl_yrs = company_characteristics %>%
#     group_by(YEAR) %>%
#     summarize(num = n()) %>%
#     arrange(desc(num))

# as_gtsummary(tbl_yrs) %>%
#   as_kable(
#     format    = "latex",
#     booktabs  = TRUE
#   ) 

```


## Daily Stock data

### Merge the data
```{r, merge_d}

# daily
reg_data_d <- merge(company_characteristics_2018, data_abnormal_returns_d, by = "NAME")  %>%
    rename("CTRY_OF_DOM_NAME" = "CTRY_OF_DOM_NAME.x") %>%
    select(-CTRY_OF_DOM_NAME.y) %>%
    arrange(NAME, DATE) %>%
    mutate(
        ABN_RET_LOG = STOCK_LOG_RETURN - NORMAL_RETURN,
        # POST_test = ifelse((DATE >= as.Date("2019-08-15")) & (DATE <= as.Date("2019-09-10'")), 1, 0),
        POST_1 = ifelse(DATE >= as.Date("2019-09-20"), 1, 0),
        POST_rb_5 = ifelse(DATE >= as.Date("2019-09-15"), 1, 0),
        POST_rb_10 = ifelse(DATE >= as.Date("2019-09-10"), 1, 0),
        DATE = as.Date(DATE),
        DATE_factor = factor(DATE),
        NAME_factor = factor(NAME),
        T_cont = as.factor(T_cont)
    )


reg_data_d <- reg_data_d  %>%
    group_by(NAME) %>%
    filter(!(is.na(ABN_RET_LOG))) %>%
    mutate(CAR = cumsum(ABN_RET_LOG)) %>%
    ungroup() %>%
    # filter(DATE > as.Date("2019-07-15"), DATE < as.Date("2019-10-10")) %>%
    filter(DATE > as.Date("2019-09-01"), DATE < as.Date("2019-10-10")) %>%
    arrange(NAME, DATE)

# View(reg_data_d)

```

### CAR Evolution per group

```{r, trend_evo_d}

car_graph_d  <- reg_data_d %>%
    group_by(T_cont, DATE) %>%
    summarize(mean_car = mean(CAR))

head(car_graph_d)


ggplot(data = car_graph_d) +
    geom_line(aes(x = DATE, y = mean_car, color = as.factor(T_cont), group = T_cont), linewidth = 0.7) +
    geom_vline(aes(xintercept = as.Date("2019-09-20")),
               color = "red", linetype = "dashed", size = 0.7) +
    scale_color_grey(start = 0.2, end = 0.8, name = "Treatment") +
    scale_x_date(date_breaks = "3 days", date_labels = "%b %d") +
    theme_bw(base_size = 12) +
    theme(
        text = element_text(family = "Times"),
        axis.text = element_text(size = 13),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_text(size = 16),
        axis.title.x = element_blank(),
        panel.grid.minor = element_blank(),
        panel.spacing = unit(1, "lines"),
        strip.text = element_text(face = "bold", size = 12),
        strip.background = element_rect(fill = "white"),
        plot.title = element_text(size = 14, face = "bold", hjust = 0)
    ) +
    labs(
        x = "Date",
        y = "Mean Cumulative Abnormal Return"
    )

ggsave("images/output/stock_d_trend.pdf", height=6, width=8)
```

### DiD
```{r, did_stock}

did_stock <- feols(CAR ~  i(T_cont, POST_1, ref = 1) | NAME_factor + DATE_factor,
               data = reg_data_d, cluster = ~NAME_factor)
summary(did_stock)

# -------- estimating acrts
acrt_did_stock <- get_acrt(did_stock)

```


### DiD for robustness 

```{r, did_stock_robustness}

# ------------ 5 days before
did_stock_rb_5 <- feols(CAR ~  i(T_cont, POST_rb_5, ref = 1) | NAME_factor + DATE_factor,
               data = reg_data_d, cluster = ~ NAME_factor)
summary(did_stock_rb_5)


# ------------ 10 days before
did_stock_rb_10 <- feols(CAR ~  i(T_cont, POST_rb_10, ref = 1) | NAME_factor + DATE_factor,
               data = reg_data_d, cluster = ~ NAME_factor)
summary(did_stock_rb_10)
```

### Event study regression
```{r, event_study_d}
event_date <- as.Date("2019-09-20")

reg_data_d_eve <- reg_data_d  %>%
    mutate(
        TimeTo_T = as.factor(difftime(reg_data_d$DATE, event_date, units = "days"))
    )


es_twfe <- feols(CAR ~ i(T_cont, TimeTo_T, ref = 1, ref2 = -1) | DATE_factor + NAME_factor,
                 cluster = ~NAME_factor, data = reg_data_d_eve)

summary(es_twfe)


# making the event study plots
stock_price_eve_2 <- eve_by_t(es_twfe, "T_cont::2", event_date = event_date, daily = TRUE)
stock_price_eve_3 <- eve_by_t(es_twfe, "T_cont::3", event_date = event_date, daily = TRUE)
stock_price_eve_4 <- eve_by_t(es_twfe, "T_cont::4", event_date = event_date, daily = TRUE)

ggsave("images/output/stock_price_eve_2.pdf", stock_price_eve_2, height = 4, width = 8)
ggsave("images/output/stock_price_eve_3.pdf", stock_price_eve_3, height = 4, width = 8)
ggsave("images/output/stock_price_eve_4.pdf", stock_price_eve_4, height = 4, width = 8)

```


## Sales data

### Merge data
```{r, merge_sales}
data_sales  <- data_sales  %>%
    filter(DATE != "2010 Q1")

# View(data_sales)

reg_data_sales <- merge(company_characteristics_2018, data_sales, by = "TYPE") %>%
    arrange(NAME.y, DATE)  %>%
    select(-YEAR.x, -YEAR.y, -QUARTER, -NAME.x)  %>%
    rename(NAME = NAME.y)  %>%
    group_by(NAME) %>%
    mutate(LOG_SALES_DIFF_4 = LOG_SALES_DIFF - lag(LOG_SALES_DIFF, 4))  %>%
    ungroup() %>%
    mutate(
        POST_1 = ifelse(DATE >= "2019 Q3", 1, 0),
        DATE_factor = factor(DATE),
        NAME_factor = factor(NAME),
        T_cont = as.factor(T_cont)
    )  %>%
    filter(DATE > "2018 Q4", DATE < "2020 Q2")

# View(reg_data_sales)

```

### Trend evolution per gro %>%up

```{r, trend_evo_sales}

trend_graph_sales  <- reg_data_sales %>%
    group_by(T_cont, DATE) %>%
    summarize(mean_sales = mean(SALES),
              mean_sales_diff = mean(LOG_SALES_DIFF_4))  

head(trend_graph_sales)


# Mean sales difference by treatment group
ggplot(data = trend_graph_sales) +
    geom_line(aes(x = DATE, y = mean_sales_diff, color = as.factor(T_cont), group = T_cont), linewidth = 0.7) +
    geom_vline(aes(xintercept = "2019 Q3"),
               color = "red", linetype = "dashed", size = 0.7) +
    scale_color_grey(start = 0.2, end = 0.8, name = "Treatment") +
    theme_bw(base_size = 12) +
    theme(
        text = element_text(family = "Times"),
        axis.text = element_text(size = 13),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_text(size = 16),
        axis.title.x = element_blank(),
        panel.grid.minor = element_blank(),
        panel.spacing = unit(1, "lines"),
        strip.text = element_text(face = "bold", size = 12),
        strip.background = element_rect(fill = "white"),
        plot.title = element_text(size = 14, face = "bold", hjust = 0)
    ) +
    labs(
        x = "Quarter",
        y = "Mean Log Sales Difference (YoY)"
    )

ggsave("images/output/sales_trend.pdf", height=6, width=8)
```


### DiD on sales data
```{r, did_sales}

did_sales <- feols(LOG_SALES_DIFF_4 ~ i(T_cont, POST_1, ref = 1) | NAME_factor + DATE_factor,
               data = reg_data_sales, cluster = ~ NAME_factor)
summary(did_sales)



# -------- estimating acrts
acrt_did_sales <- get_acrt(did_sales)
```


### Event Study - sales

```{r, sales_eve}
event_Q <- as.yearqtr("2019 Q3") # nolint

reg_data_eve <- reg_data_sales %>%
    mutate(
        # convert your dates to yearqtr
        DATE_Q   = as.yearqtr(DATE),
        event_Q  = as.yearqtr(event_Q),
        # numeric difference in quarters
        TimeTo_T = as.factor(DATE_Q - event_Q)
    )

# View(reg_data_eve)
sales_eve <- feols(LOG_SALES_DIFF_4 ~ i(T_cont, TimeTo_T, ref = 1, ref2=-0.25) | DATE_factor + NAME_factor,
                   cluster = ~NAME_factor, data = reg_data_eve)
summary(sales_eve)

# making the plots 
sales_eve_2 <- eve_by_t(sales_eve, "T_cont::2", event_Q, qtr = TRUE)
sales_eve_3 <- eve_by_t(sales_eve, "T_cont::3", event_Q, qtr = TRUE)
sales_eve_4 <- eve_by_t(sales_eve, "T_cont::4", event_Q, qtr = TRUE)

ggsave("images/output/sales_eve_2.pdf", sales_eve_2, height = 4, width = 8)
ggsave("images/output/sales_eve_3.pdf", sales_eve_3, height = 4, width = 8)
ggsave("images/output/sales_eve_4.pdf", sales_eve_4, height = 4, width = 8)

```

## Making summary tables for latex

```{r, sum_tbl}
# main table
etable(did_stock, did_sales, style.tex = style.tex("aer"), tex = TRUE)


# testing the no-anticipation assumption 
etable(did_stock_rb_5, did_stock_rb_10, style.tex = style.tex("aer"), tex = TRUE)

```

## Other 
If a company makes specific claims in their reports they would probably make them in marketing - better perceived by consumers?
Think about different justifications, e.g. reports look at the past years, 
companies make tv commercials that can include sustainability information
