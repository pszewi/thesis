---
title: "Data Analysis for Thesis"
author: "Jakub Przewoski"
date: "2025-04-28"
output: html_document
---

# Description
In this file I will be writing down the methodological notes for my thesis and I will also compute here my experiment. 



## Experiment-setup:
I am trying to investigate whether firms can utilize greenwashing as a profitable strategy. In order to do that I want to create a measure of greenwashing by following *(Lagasio, 2024)*. Furthermore, I will utilize a natural experiment (DiD) to search for causality.


## Experiment notes:
* Use a DiD with continuous treatment to see whether greenwashing more makes you profit more
* For applying treatment, use the level of greenwashing from 2017/2018
    * The justification is that I need to assume that the treatment level was constant which implies that I should have as little time as possible between the treatment assignment and my shock 
    * I have computed both the 2017 and the 2018 Greenwashing index.
* For the shock, use September 2019, as that's when there was a lot of buzz around another IPCC report and Fridays For Future movement
    * use cumulative abnormal returns as the outcome variable 
    * use sales as a secondary outcome variable

Some pictures showing google trends around specific queries around that time:

<div>
<img src="../images/ipcc_trends_2008_2021.png" width="600"/>
</div>


<div>
<img src="../images/climate_trends_2004_2022.png" width="600"/>
</div>


----

## Some links:
* [IPPC Wikipedia Page](https://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change)
* [6th IPCC Assessment](https://en.wikipedia.org/wiki/IPCC_Sixth_Assessment_Report)
* [IPCC Special Report from 2019](https://en.wikipedia.org/wiki/Special_Report_on_Climate_Change_and_Land)
* [Fridays For Future movement Wiki](https://en.wikipedia.org/wiki/Fridays_for_Future)
* [List of school climate strikes](https://en.wikipedia.org/wiki/List_of_school_climate_strikes)


----

# Code

## Loading libraries


```{r, libs, message = FALSE}
library(httpgd)
library(tidyverse)
library(data.table)
library(ggplot2)
library(fixest)
library(stargazer)
library(readxl)
```

## Trends in articles data

```{r, query_trends}
trends <- read_excel("data/nyt_query.xlsx")

trends <- trends %>%
    group_by(Query) %>%
    mutate(new_art = num - lag(num),
           log_new_art = log(new_art),
           log_diff = log_new_art - lag(log_new_art)) %>%
    ungroup() %>%
    mutate(Date = as.Date(trends$Date))  %>%
    filter(Date > as.Date("2010-01-01"))

# View(trends)

ggplot(data = trends) +
    geom_line(aes(x = Year, y = new_art/1000), linewidth = 1) +
    facet_wrap(~Query, scales = "free_y", ncol = 2) +
    scale_x_continuous(breaks = seq(2010, 2023, 2)) +
    geom_vline(aes(xintercept = 2019),
               color = "red", linetype = "dashed", size = 0.8) +
    theme_bw(base_size = 12) +
    theme(
        axis.text = element_text(size = 13),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_text(size = 16, face = "bold"),
        panel.grid.minor = element_blank(),
        panel.spacing = unit(1, "lines"),
        # legend.position = "none",
        strip.text = element_text(face = "bold", size = 12),
        strip.background = element_rect(fill = "white"),
        plot.title = element_text(size = 14, face = "bold", hjust = 0),
        plot.subtitle = element_text(size = 12, hjust = 0),
        plot.caption = element_text(size = 10, hjust = 0, face = "italic")
    ) +
    scale_color_grey(start = 0.2, end = 0.8) +
    labs(
        # title = "Growth in New York Times Coverage by Topic",
        # subtitle = "Number of new articles published per year (2010-2023)",
        x = "Year",
        y = "New Articles (thousands)",
        # caption = "Source: New York Times"
    )

ggsave("images/output/nyt_articles.png")

# ggplot(data = trends) +
#     geom_line(aes(x = Year, y = log_diff), linewidth = 1) +
#     facet_wrap(~Query, scales = "free") +
#     scale_x_continuous(breaks = seq(2008, 2023, 1)) +
#     geom_vline(aes(xintercept = 2019),
#                color = "red", linetype = "dashed", size = 1) +
#     theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
#     labs(
#         title = "Change in new New York Times articles, per topic",
#         x = "Year",
#         y = "\u0394 in new articles",
#         caption = "Source: New York Times"
#     )

```



## Loading Data for Analysis

First loading here data at the weekly frequency as it seems the most reasonable.

```{r,data, message = FALSE}
company_characteristics <- read_csv("data/output/company_characteristics.csv")
greenwashing_ind_2017  <- read_csv("data/text_processing/greenwashing_ind_2017.csv")
greenwashing_ind_2018  <- read_csv("data/text_processing/greenwashing_ind_2018.csv")


data_abnormal_returns_w <- read_csv("data/output/data_abnormal_returns_w.csv")
data_abnormal_returns_d <- read_csv("data/output/data_abnormal_returns_d.csv")
data_sales  <- read_csv("data/output/data_sales.csv")

```

Adjusting the universal data (needed for all regressions)

```{r, char_green_data}

# get rid of an observation with a missing indicator and likely faulty outliers
greenwashing_ind_2017 <- greenwashing_ind_2017 %>%
    filter(NAME != "Genmab A/S")  %>%
    arrange(CLIMATE_REL)
# View(greenwashing_ind_2017)

# get rid of an observation with a missing indicator and likely faulty outliers
greenwashing_ind_2018 <- greenwashing_ind_2018 %>%
    filter(!(NAME %in% c("Genmab A/S", "Ashland Inc.", "Fletcher Building Limited", "Ethan Allen Interiors Inc.",
                         "Equifax Inc.", "Sagicor Financial Company Ltd.", "Korn/Ferry International",
                         "Spark NZ", "Covestro AG")),
           CLIMATE_REL > 2)  %>%
    mutate(GREEN_IND_2 = NON_SPEC / CLIMATE_REL,
        T_cont = case_when(
            GREEN_IND_2 < quantile(GREEN_IND_2, prob = c(0.25)) ~ 1,
            (GREEN_IND_2 >= quantile(GREEN_IND_2, prob = c(0.25))) &
                (GREEN_IND_2 < quantile(GREEN_IND_2, prob = c(0.5))) ~ 2,
            (GREEN_IND_2 >= quantile(GREEN_IND_2, prob = c(0.5))) &
                (GREEN_IND_2 < quantile(GREEN_IND_2, prob = c(0.75))) ~ 3,
            GREEN_IND_2 >= quantile(GREEN_IND_2, prob = c(0.75)) ~ 4
        ),
        T_ = ifelse(GREEN_IND > mean(GREEN_IND), 1, 0)
    )
# View(greenwashing_ind_2018)


company_characteristics  <- company_characteristics  %>% filter(YEAR == 2018)
company_characteristics_2018 <- merge(company_characteristics, greenwashing_ind_2018,
                                      by.x = "NAME_SCRAPED", by.y = "NAME") %>%
    select(c("TYPE", "NAME", "BOURSE_NAME", "ICB_INDUSTRY_NAME", "ICB_SECTOR_NAME", "CTRY_OF_DOM_NAME",
             "CTRY_OF_INC_NAME", "EMPLOYEES", "YEAR", "GREEN_IND_2", "T_", "T_cont")) %>%
    filter(!duplicated(NAME))

```

## Descriptive statistics

```{r, desc_stats, asis=TRUE}

ggplot(greenwashing_ind_2018) +
    geom_density(aes(x = GREEN_IND), color = "red") +
    geom_density(aes(x = GREEN_IND_2), color = "blue")





ggplot(data = company_characteristics_2018,
       #    aes(x = factor(ICB_INDUSTRY_NAME))) +
       aes(x = factor(ICB_INDUSTRY_NAME), fill = factor(T_))) +
    # geom_bar(fill   = "steelblue", color  = "black", width  = 0.7) +
    geom_bar(position = "fill", width = 0.7) +
    labs(
         x     = "Category",
         y     = "Count",
         title = "Distribution of Categories", 
         fill="Treatment") +
    theme_bw(base_size = 14) +
    theme(
          panel.grid.major    = element_blank(),
          panel.grid.minor    = element_blank(),
          axis.text.x         = element_text(angle = 45, hjust = 1),
          plot.title          = element_text(hjust = 0.5, face = "bold"),
          axis.title          = element_text(face = "bold"))



# here I made some tables for the latex document (descriptive statistics)
# library(gtsummary)

# tbl_ltx = company_characteristics_2018 %>%
#     select(CTRY_OF_DOM_NAME, T_cont) %>%
#     tbl_summary(
#                 by = T_cont,
#                 statistic = list(CTRY_OF_DOM_NAME ~ "{n} ({p}%)")) %>%
#     add_overall() %>%
#     modify_header(
#                   label     = "Number of ...",
#                   all_stat_cols() ~ "{level} (N = {n})") %>%
#     modify_table_body(~.x %>% relocate(stat_0, stat_2, stat_1, .after = label))


# tbl_ltx %>%
#   as_kable(
#     format    = "latex",
#     booktabs  = TRUE
#   ) 

# # number of reports per year
# tbl_yrs = company_characteristics %>%
#     group_by(YEAR) %>%
#     summarize(num = n()) %>%
#     arrange(desc(num))

# as_gtsummary(tbl_yrs) %>%
#   as_kable(
#     format    = "latex",
#     booktabs  = TRUE
#   ) 

```


## Stock data

#### Weekly 

Merging the data into a single dataset for regressing
```{r, merge_w}

# weekly
reg_data_w <- merge(company_characteristics_2018, data_abnormal_returns_w, by = "NAME")  %>%
    rename("CTRY_OF_DOM_NAME" = "CTRY_OF_DOM_NAME.x") %>%
    select(-CTRY_OF_DOM_NAME.y) %>%
    mutate(
        ABN_RET_LOG = STOCK_LOG_RETURN - NORMAL_RETURN,
        POST = ifelse(DATE >= as.Date("2019-08-08"), 1, 0),
        TxPOST = as.factor(T_ * POST),
        DATE = as.Date(DATE),
        DATE_factor = factor(DATE),
        NAME_factor = factor(NAME)
    )

reg_data_w <- reg_data_w  %>%
    group_by(NAME) %>%
    filter(!(is.na(ABN_RET_LOG))) %>%
    mutate(CAR = cumsum(ABN_RET_LOG)) %>%
    ungroup()  %>%
    filter(DATE < as.Date("2019-09-08")) %>%
    arrange(NAME, DATE)


```

## CAR Evolution per group

```{r, trend_evo_w}

car_graph_w  <- reg_data_w %>%
    group_by(T_, DATE) %>%
    summarize(mean_car = mean(CAR))

head(car_graph_w)


ggplot() + geom_line(data = car_graph_w, aes(x = DATE, y = mean_car, color = as.factor(T_)))

```

#### DiD
```{r, did_w}

print("------------")
model <- feols(CAR ~  TxPOST | NAME_factor + DATE_factor,
               data = reg_data_w, cluster = ~ NAME_factor)
summary(model)


```


## Event study regression
```{r, event_study_w}
event_date_1 <- as.Date("2019-08-09")
event_date_2 <- as.Date("2019-09-20")

reg_data_w_eve <- reg_data_w  %>%
    mutate(
        TimeTo_T_1 = as.numeric(difftime(reg_data_w$DATE, event_date_1, units = "weeks")),
        TimeTo_T_2 = as.numeric(difftime(reg_data_w$DATE, event_date_2, units = "weeks"))
    )

# View(reg_data_w_eve)

# ------------ FIRST SHOCK
mod_twfe <- feols(CAR ~ i(TimeTo_T_1, T_, ref = -1) | DATE_factor + NAME_factor,
                  cluster = ~NAME_factor, data = reg_data_w_eve)

iplot(mod_twfe,
      xlab = "Time to treatment",
      main = "Event study: Treatment (TWFE)")


# # ----------- SECOND SHOCK
# reg_data_w_eve <- reg_data_w_eve  %>%
#     filter(DATE > as.Date("2019-09-10"))

# mod_twfe <- feols(CAR ~ i(TimeTo_T_2, T_, ref = -1) | DATE_factor + NAME_factor,
#                   cluster = ~NAME_factor, data = reg_data_w_eve)

# iplot(mod_twfe,
#       xlab = "Time to treatment",
#       main = "Event study: Treatment (TWFE)")
```


## Diff-in-Diff regression

#### Daily



```{r, merge_d}

# daily
reg_data_d <- merge(company_characteristics_2018, data_abnormal_returns_d, by = "NAME")  %>%
    rename("CTRY_OF_DOM_NAME" = "CTRY_OF_DOM_NAME.x") %>%
    select(-CTRY_OF_DOM_NAME.y) %>%
    arrange(NAME, DATE) %>%
    mutate(
        ABN_RET_LOG = STOCK_LOG_RETURN - NORMAL_RETURN,
        POST_1 = ifelse(DATE >= as.Date("2019-08-08"), 1, 0),
        POST_2 = ifelse(DATE >= as.Date("2019-09-20"), 1, 0),
        TxPOST_1 = as.factor(T_ * POST_1),
        TxPOST_2 = as.factor(T_ * POST_2),
        DATE = as.Date(DATE),
        DATE_factor = factor(DATE),
        NAME_factor = factor(NAME)
    )


reg_data_d <- reg_data_d  %>%
    group_by(NAME) %>%
    filter(!(is.na(ABN_RET_LOG))) %>%
    mutate(CAR = cumsum(ABN_RET_LOG)) %>%
    ungroup() %>%
    filter(DATE > as.Date("2019-07-15"), DATE < as.Date("2019-08-21")) %>%
    # filter(DATE > as.Date("2019-09-10"), DATE < as.Date("2019-10-01")) %>%
    arrange(NAME, DATE)

View(reg_data_d)

```

## CAR Evolution per group

```{r, trend_evo_d}

car_graph_d  <- reg_data_d %>%
    group_by(T_, DATE) %>%
    summarize(mean_car = mean(CAR))

head(car_graph_d)


ggplot() + geom_line(data = car_graph_d, aes(x = DATE, y = mean_car, color = as.factor(T_)))

```

#### DiD
```{r, did_d}

print("------------")
model <- feols(CAR ~  TxPOST_1 | NAME_factor + DATE_factor,
               data = reg_data_d, cluster = ~ NAME_factor)
summary(model)

# ---------------------------
# did but with

model <- feols(CAR ~  i(T_cont, POST_1, ref = 1) | ICB_INDUSTRY_NAME^DATE_factor,
               data = reg_data_d, cluster = ~ NAME_factor)
summary(model)

```


## Event study regression
```{r, event_study_d}
event_date_1 <- as.Date("2019-08-08")
event_date_2 <- as.Date("2019-09-20")

reg_data_d_eve <- reg_data_d  %>%
    mutate(
        TimeTo_T_1 = as.numeric(difftime(reg_data_d$DATE, event_date_1, units = "days")),
        TimeTo_T_2 = as.numeric(difftime(reg_data_d$DATE, event_date_2, units = "days"))
    )

# View(reg_data_d_eve)

# ------------ FIRST SHOCK
mod_twfe <- feols(CAR ~ i(TimeTo_T_1, T_, ref = -1) | DATE_factor + NAME_factor,
                  cluster = ~NAME_factor, data = reg_data_d_eve)

iplot(mod_twfe,
      xlab = "Time to treatment",
      main = "Event study: Treatment (TWFE)")


# ----------- SECOND SHOCK
reg_data_d_eve <- reg_data_d_eve  %>%
    filter(DATE > as.Date("2019-09-10"))

mod_twfe <- feols(CAR ~ i(TimeTo_T_2, T_, ref = -1) | DATE_factor + NAME_factor,
                  cluster = ~NAME_factor, data = reg_data_d_eve)

iplot(mod_twfe,
      xlab = "Time to treatment",
      main = "Event study: Treatment (TWFE)")
```


## Sales data

```{r, merge_sales}
data_sales  <- data_sales  %>%
    filter(DATE != "2010 Q1")

# View(data_sales)

reg_data_sales <- merge(company_characteristics_2018, data_sales, by = "TYPE") %>%
    arrange(NAME.y, DATE)  %>%
    select(-YEAR.x, -YEAR.y, -QUARTER, -NAME.x)  %>%
    rename(NAME = NAME.y)  %>%
    group_by(NAME) %>%
    mutate(LOG_SALES_DIFF_4 = LOG_SALES_DIFF - lag(LOG_SALES_DIFF, 4))  %>%
    ungroup() %>%
    mutate(
        POST_1 = ifelse(DATE >= "2019 Q3", 1, 0),
        TxPOST = as.factor(T_ * POST_1),
        DATE_factor = factor(DATE),
        NAME_factor = factor(NAME)
    )  %>%
    filter(DATE >= "2017 Q3", DATE < "2021 Q1")

# View(reg_data_sales)

nrow(unique(reg_data_sales[reg_data_sales$T_ == 1, "NAME"]))
nrow(unique(reg_data_sales[reg_data_sales$T_ == 0, "NAME"]))

```

#### Trend evolution per group

```{r, trend_evo_sales}

trend_graph_sales  <- reg_data_sales %>%
    group_by(T_cont, DATE) %>%
    summarize(mean_sales = mean(SALES),
              mean_sales_diff = mean(LOG_SALES_DIFF_4))

head(trend_graph_sales)


# Mean sales by treatment group
ggplot(data = trend_graph_sales) +
    geom_line(aes(x = DATE, y = mean_sales, color = as.factor(T_cont), group = T_cont), linewidth = 1) +
    geom_vline(aes(xintercept = "2019 Q3"),
               color = "red", linetype = "dashed", size = 0.8) +
    scale_color_grey(start = 0.2, end = 0.8, name = "Treatment") +
    theme_bw(base_size = 12) +
    theme(
        axis.text = element_text(size = 13),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_text(size = 16, face = "bold"),
        panel.grid.minor = element_blank(),
        panel.spacing = unit(1, "lines"),
        strip.text = element_text(face = "bold", size = 12),
        strip.background = element_rect(fill = "white"),
        plot.title = element_text(size = 14, face = "bold", hjust = 0)
    ) +
    labs(
        x = "Quarter",
        y = "Mean Sales"
    )

# Mean sales difference by treatment group
ggplot(data = trend_graph_sales) +
    geom_line(aes(x = DATE, y = mean_sales_diff, color = as.factor(T_cont), group = T_cont), linewidth = 1) +
    geom_vline(aes(xintercept = "2019 Q3"),
               color = "red", linetype = "dashed", size = 0.8) +
    scale_color_grey(start = 0.2, end = 0.8, name = "Treatment") +
    theme_bw(base_size = 12) +
    theme(
        axis.text = element_text(size = 13),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_text(size = 16, face = "bold"),
        panel.grid.minor = element_blank(),
        panel.spacing = unit(1, "lines"),
        strip.text = element_text(face = "bold", size = 12),
        strip.background = element_rect(fill = "white"),
        plot.title = element_text(size = 14, face = "bold", hjust = 0)
    ) +
    labs(
        x = "Quarter",
        y = "Mean Log Sales Difference (Year-over-Year)"
    )

```


#### DiD on sales data
```{r, did_sales}
library(zoo)


print("------------")
model <- feols(LOG_SALES_DIFF_4 ~ TxPOST | NAME_factor + DATE_factor,
               data = reg_data_sales, cluster = ~ NAME_factor)
summary(model)

event_Q <- "2019 Q3" # nolint

reg_data_eve <- reg_data_sales %>%
    mutate(
        # convert your dates to yearqtr
        DATE_Q   = as.yearqtr(DATE),
        event_Q  = as.yearqtr(event_Q),
        # numeric difference in quarters
        TimeTo_Q = as.numeric(DATE_Q - event_Q)
    )

# View(reg_data_eve)
mod_twfe <- feols(LOG_SALES_DIFF_4 ~ i(TimeTo_Q, T_, ref = -0.25) | DATE_factor + NAME_factor,
                  cluster = ~NAME_factor, data = reg_data_eve)

mod_twfe <- feols(LOG_SALES_DIFF_4 ~ i(TimeTo_Q, T_cont, ref = -0.25) | DATE_factor + NAME_factor,
                  cluster = ~NAME_factor, data = reg_data_eve)
summary(mod_twfe)
iplot(mod_twfe,
      xlab = "Time to treatment",
      main = "Event study: Treatment (TWFE)")


```

If a company makes specific claims in their reports they would probably make them in marketing - better perceived by consumers?
Think about different justifications, e.g. reports look at the past years, 
companies make tv commercials that can include sustainability information


## Testing continuous treatment on sales

```{r}

reg_data_sales <- reg_data_sales %>%
    mutate(
        T_cont = as.factor(T_cont)
    )  %>%
    filter(DATE <=  as.yearqtr("2020 Q4"))

print("------------")
model <- feols(LOG_SALES_DIFF_4 ~ i(T_cont, POST_1, ref = 1) | NAME_factor + DATE_factor,
               data = reg_data_sales, cluster = ~ NAME_factor)
summary(model)


```