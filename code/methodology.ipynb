{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc75f8e5",
   "metadata": {},
   "source": [
    "# Methodology and Analysis Notebook\n",
    "#### In this notebook I note my methods and ideas as I go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1f860",
   "metadata": {},
   "source": [
    "#### Experiment-setup:\n",
    "\n",
    "I am trying to investigate whether firms can utilize greenwashing as a profitable strategy. In order to do that I want to create a measure of greenwashing by following *(Lagasio, 2024)*. Furthermore, I will utilize a natural experiment (DiD) to search for causality.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Experiment notes:\n",
    "* Use a DiD with continuous treatment to see whether greenwashing more makes you profit more\n",
    "* For applying treatment, use the level of greenwashing from 2014/2015\n",
    "    * The justification is that in 2013/2014 the previous IPCC report was released \n",
    "    * Since that created a bit of buzz, it may have allowed firms to start greenwashing\n",
    "    * I could also just use the classification just before the 2018/2019 shock as that's closer & more data\n",
    "    * The 2009/2010 shock is explained by the Copenhagen Accord signed in the fall 2009\n",
    "    <div>\n",
    "    <img src=\"../img/ipcc_trends_2008_2021.png\" width=\"600\"/>\n",
    "    </div>\n",
    "\n",
    "* For the shock, use September 2019, as that's when there was a lot of buzz around another IPCC report and Fridays For Future movement\n",
    "    <div>\n",
    "    <img src=\"../img/climate_trends_2004_2022.png\" width=\"600\"/>\n",
    "    </div>\n",
    "\n",
    "* use cumulative abnormal returns as the outcome variable \n",
    "* use sales as a secondary outcome variable\n",
    "\n",
    "----\n",
    "#### Some links:\n",
    "* [IPPC Wikipedia Page](https://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change)\n",
    "* [6th IPCC Assessment](https://en.wikipedia.org/wiki/IPCC_Sixth_Assessment_Report)\n",
    "* [IPCC Special Report from 2019](https://en.wikipedia.org/wiki/Special_Report_on_Climate_Change_and_Land)\n",
    "* [Fridays For Future movement Wiki](https://en.wikipedia.org/wiki/Fridays_for_Future)\n",
    "* [List of school climate strikes](https://en.wikipedia.org/wiki/List_of_school_climate_strikes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea8c75",
   "metadata": {},
   "source": [
    "Total Return Index from Datastream (commented out)\n",
    "<!-- > Total Return Index from data_greenwashingstream:\n",
    "> The Return Index shows the growth in value of a security over a specified period, assuming that dividends are re-invested to purchase additional shares of the security at the closing price (P) on the dividend ex-date.\n",
    "> \n",
    "> For all markets except Canada and the US, dividend payment data (DDE) is only available from 1988, for securities listed before this point the return index is calculated using the dividend yield (DY). This method adds an increment of 1/260th part of the dividend yield to the price each weekday. There are assumed to be 260 weekdays in a year, market holidays are ignored.\n",
    "> \n",
    "> Method 1 (using annualised dividend yield)\n",
    "> \n",
    "> RI on the BDATE =100, then:\n",
    "> \n",
    "> cid:image001.gif@01CC34D5.05D81F30\n",
    "> \n",
    "> Where:\n",
    "> \n",
    "> cid:image002.gif@01CC34D5.05D81F30 = return index on day t\n",
    "> \n",
    "> cid:image003.gif@01CC34D5.05D81F30= return index on previous day\n",
    "> \n",
    "> cid:image004.gif@01CC34D5.05D81F30 = price index on day t\n",
    "> \n",
    "> cid:image005.gif@01CC34D5.05D81F30 = price index on previous day\n",
    "> \n",
    "> cid:image006.gif@01CC34D5.05D81F30 = dividend yield % on day t\n",
    "> \n",
    ">  N = number of working days in the year (taken to be 260)\n",
    "> \n",
    "> For securities listed before 1988 the RI calculation switches from the dividend yield method to using the dividend payment data, for example the first recorded dividend for BP is 15/08/88 – on this date the RI calculation switches from the dividend yield method to using the dividend payment data. This method represents a more accurate measure of the security’s growth in which the discrete quantity of dividend paid is added to the price on the ex-date of the payment\n",
    "> \n",
    "> Method 2 (using dividend payment data)\n",
    "> \n",
    "> RI on the BDATE =100, then:\n",
    "> \n",
    ">  cid:image007.gif@01CC34D5.05D81F30\n",
    "> \n",
    ">  except when t = ex-date of the dividend payment Dt then:\n",
    "> \n",
    "> cid:image011.gif@01CC34D6.0623B1B0\n",
    "> \n",
    "> Where:\n",
    "> \n",
    ">  cid:image008.gif@01CC34D5.05D81F30 = price on ex-date\n",
    "> \n",
    "> = price on previous day\n",
    "> \n",
    ">  cid:image010.gif@01CC34D5.05D81F30 = dividend payment associated with ex-date t\n",
    "> \n",
    "> The calculation ignores tax and re-investment charges.\n",
    "> \n",
    "> Canadian and US securities are not affected by the dual calculation method, dividend payment data is available from 1973 – the Datastream market inception date for both markets.\n",
    "> \n",
    "> Securities listed after 1988 automatically use RI calculation method 2, dividend payment data.\n",
    "> \n",
    "> Adjusted closing prices are used throughout to determine price index and hence return index.\n",
    "> \n",
    "> Note: where the dividend payment data history contains a mixture of gross and net dividends the annualised dividend yield (method 1 above) is used in order to achieve a consistent growth measure. The net and gross markers can be identified using the datatype DTAX (tax marker). To display the total return using the dividend payment data (method 2) in these cases, the alternative total return datatype RZ (Return Index - As Paid) may be used. RZ uses the dividend payment data calculation method irrespective of the tax markers.\n",
    "> \n",
    "> For UK companies the RI includes a tax credit on the dividend until it was abolished in April 2004. Prior to that time dividends as announced by the company are grossed up to include to the credit in the RI calculation. The rate used varies over time, the last rate being 10% in the period April 1999 to April 2004. To see a return index measure which does not include tax credits, the datatype RN (Return Index – Net) should be used. The RN calculation is the same as RI except the dividends used are as announced by the company and not grossed up for the tax credits. It follows that since April 2004 the performance of RI and RN for UK securities is the same. RN is available for UK securities only. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77006b17",
   "metadata": {},
   "source": [
    "#### Data Processing\n",
    "\n",
    "First reading in pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce218eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymupdf\n",
    "from datetime import datetime\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "from itertools import product\n",
    "import spacy\n",
    "from funcs import ExtractNameYear, ExtractFileName, ExtractAllText\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c16e8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data_greenwashing = pd.read_excel('../data/LSEG data/matched_final.xlsx', sheet_name='companies')\n",
    "data_returns = pd.read_excel('../data/LSEG data/matched_final.xlsx', sheet_name='indicators_1')\n",
    "data_indices = pd.read_excel('../data/LSEG data/matched_final.xlsx', sheet_name='INDICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing returns and merging to greenwashing\n",
    "data_returns = data_returns[(data_returns['Retrieving...'].str.contains(\"PRICE INDEX\")) | (data_returns['Retrieving...'].str.contains(\"TOT RETURN IND\"))]\n",
    "data_returns.loc[:,\"NAME\"] = data_returns.loc[:,\"Retrieving...\"].str.removesuffix(' - PRICE INDEX').str.removesuffix(\" - TOT RETURN IND\")\n",
    "data_returns.loc[:, \"VARIABLE\"] = np.where(data_returns.loc[:,\"Retrieving...\"].str.contains(\" - PRICE INDEX\"), 'PRICE INDEX', pd.NA)\n",
    "data_returns.loc[:, \"VARIABLE\"] = np.where(data_returns.loc[:,\"Retrieving...\"].str.contains(\" - TOT RETURN IND\"), 'TOT RETURN IND', data_returns.loc[:, \"VARIABLE\"])\n",
    "\n",
    "\n",
    "data_returns.drop([\"Retrieving...\"], axis=1, inplace=True)\n",
    "data_returns = pd.melt(data_returns, id_vars=['NAME', \"VARIABLE\"]).pivot_table(index=['NAME', 'variable'], columns='VARIABLE', values='value').reset_index().rename(columns={'variable':'DATE'})\n",
    "data_returns = data_returns.merge(data_greenwashing[['NAME', 'CTRY_OF_DOM_NAME']], how=\"left\", on=\"NAME\")\n",
    "\n",
    "data_returns[\"DATE\"] = data_returns[\"DATE\"].dt.date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_indices.loc[:,\"NAME\"] = data_indices.loc[:,\"Retrieving....1\"].str.removesuffix(' - PRICE INDEX').str.removesuffix(\" - TOT RETURN IND\")\n",
    "data_indices.loc[:, \"VARIABLE\"] = np.where(data_indices.loc[:,\"Retrieving....1\"].str.contains(\" - PRICE INDEX\"), 'PRICE INDEX', pd.NA)\n",
    "data_indices.loc[:, \"VARIABLE\"] = np.where(data_indices.loc[:,\"Retrieving....1\"].str.contains(\" - TOT RETURN IND\"), 'TOT RETURN IND', data_indices.loc[:, \"VARIABLE\"])\n",
    "\n",
    "data_indices.drop(columns=['Retrieving...', 'Retrieving....1', 'ISIN CODE'], inplace=True)\n",
    "\n",
    "data_indices = pd.melt(data_indices, id_vars=['NAME', \"VARIABLE\"]).pivot_table(index=['NAME', 'variable'], columns='VARIABLE', values='value').reset_index().rename(columns={'variable':'DATE'})\n",
    "\n",
    "data_indices[\"DATE\"] = data_indices[\"DATE\"].dt.date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a53257c",
   "metadata": {},
   "source": [
    "#### Calculating abnormal returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea8f1a",
   "metadata": {},
   "source": [
    "Here I calculate abnormal returns and cumulative abnormal returns. In order to do that, I benchmark the realized returns of each of the companies against their expected return. The market model is estimated in order to assess the expected return. The market for each security is chosen with respect to the primary market of operations of each company (S&P for the USA/Canada, MSCI EUROPE for Europe, ... **Work-in-progress, need to get indices for more geographical regions**). Finally, I chose to use the price return on equity only (instead of total return including re-invested dividends - they don't really matter over such a short period anyway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8378a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see the market from which most of the companies come from (\"Country of Domicile\")\n",
    "print(data_greenwashing['CTRY_OF_DOM_NAME'].value_counts(normalize=True))\n",
    "\n",
    "# creating monthly returns for equities\n",
    "data_returns.dropna(subset='PRICE INDEX', inplace=True)\n",
    "data_returns['STOCK_PCT_RETURN'] = data_returns.groupby(by='NAME')['PRICE INDEX'].apply(pd.Series.pct_change).reset_index()['PRICE INDEX']\n",
    "data_returns = data_returns[['NAME', 'CTRY_OF_DOM_NAME', 'DATE','STOCK_PCT_RETURN']]\n",
    "\n",
    "# making sure that all firms have data starting from 2014\n",
    "full_index = pd.DataFrame(product(data_returns[\"NAME\"].unique(), data_returns[\"DATE\"].unique()))\n",
    "full_index.columns = [\"NAME\", \"DATE\"]\n",
    "\n",
    "data_returns = full_index.merge(data_returns, how=\"left\", on=['NAME', 'DATE'])\n",
    "\n",
    "# creating monthly returns for indices\n",
    "data_indices['INDEX_PCT_RETURN'] = data_indices.groupby(by='NAME')['PRICE INDEX'].apply(pd.Series.pct_change).reset_index()['PRICE INDEX']\n",
    "data_indices = data_indices[['NAME', 'DATE','INDEX_PCT_RETURN']]\n",
    "data_indices.dropna(subset='INDEX_PCT_RETURN', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a216200",
   "metadata": {},
   "source": [
    "Checking for duplicated data in the companies. It seems that some have been duplicated but mostly due to having a subsidiary in the same name/having multiple pages etc. \n",
    "Some of them are also the wrong company. There's little enough problems though, that it seems like it could be dropped (16 companies like that) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_returns[['NAME', 'DATE']].value_counts())\n",
    "\n",
    "data_returns.drop_duplicates(subset=['NAME', 'DATE'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since 2014-01-01 was the initial time period, I'm dropping it\n",
    "data_returns = data_returns[data_returns[\"DATE\"] != datetime.strptime('2014-01-01', '%Y-%m-%d').date()]\n",
    "\n",
    "# set the estimation window\n",
    "date_start = datetime.strptime('2010-01-01', '%Y-%m-%d').date()\n",
    "date_end = datetime.strptime('2018-09-01', '%Y-%m-%d').date()\n",
    "\n",
    "# company related parameters\n",
    "company_array = data_returns[['NAME', 'CTRY_OF_DOM_NAME']].drop_duplicates().values\n",
    "market_dict = {\n",
    "                # temporarily assigned to S&P 500\n",
    "                \"UNITED STATES\":\"S&P 500 COMPOSITE\",\n",
    "                \"CANADA\":\"S&P 500 COMPOSITE\",\n",
    "                \"BERMUDA\":\"S&P 500 COMPOSITE\",\n",
    "                \"CAYMAN ISLANDS\":\"S&P 500 COMPOSITE\",\n",
    "                \n",
    "                # temporarily assigned to MSCI europe\n",
    "                \"UNITED KINGDOM\":\"MSCI EUROPE U$\",\n",
    "                \"IRELAND\":\"MSCI EUROPE U$\",\n",
    "                \"SWITZERLAND\":\"MSCI EUROPE U$\",\n",
    "                \"NETHERLANDS\":\"MSCI EUROPE U$\",\n",
    "                \"GREECE\":\"MSCI EUROPE U$\",\n",
    "                \"GERMANY\":\"MSCI EUROPE U$\",\n",
    "                \"BELGIUM\":\"MSCI EUROPE U$\",\n",
    "                \"DENMARK\":\"MSCI EUROPE U$\",\n",
    "                \"MONACO\":\"MSCI EUROPE U$\",\n",
    "                \"LUXEMBOURG\":\"MSCI EUROPE U$\",\n",
    "                \"FRANCE\":\"MSCI EUROPE U$\",\n",
    "                \"SWEDEN\":\"MSCI EUROPE U$\",\n",
    "                \"ISLE OF MAN\":\"MSCI EUROPE U$\",\n",
    "                \"SPAIN\":\"MSCI EUROPE U$\",\n",
    "                \"FINLAND\":\"MSCI EUROPE U$\",\n",
    "                \"ROMANIA\":\"MSCI EUROPE U$\",\n",
    "                \"ITALY\":\"MSCI EUROPE U$\",\n",
    "                \"AUSTRIA\":\"MSCI EUROPE U$\",\n",
    "                \"JERSEY\":\"MSCI EUROPE U$\",\n",
    "                \"GUERNSEY\":\"MSCI EUROPE U$\",\n",
    "                \n",
    "                # temporarily assigned to MSCI world\n",
    "                \"MEXICO\":\"MSCI WORLD U$\",\n",
    "                \"PAPUA NEW GUINEA\":\"MSCI WORLD U$\",\n",
    "                \"PUERTO RICO\":\"MSCI WORLD U$\",\n",
    "                \"PANAMA\":\"MSCI WORLD U$\",\n",
    "                \"COLOMBIA\":\"MSCI WORLD U$\",\n",
    "                \"BRAZIL\":\"MSCI WORLD U$\",\n",
    "                \"CHILE\":\"MSCI WORLD U$\",\n",
    "                \"PERU\":\"MSCI WORLD U$\",\n",
    "                \"URUGUAY\":\"MSCI WORLD U$\",\n",
    "                \"ARGENTINA\":\"MSCI WORLD U$\",\n",
    "                \"COSTA RICA\":\"MSCI WORLD U$\",\n",
    "                \"TURKEY\":\"MSCI WORLD U$\",\n",
    "                \"ISRAEL\":\"MSCI WORLD U$\",\n",
    "                \"KAZAKHSTAN\":\"MSCI WORLD U$\",\n",
    "                \"UNITED ARAB EMIRATES\":\"MSCI WORLD U$\",\n",
    "                \"SOUTH AFRICA\":\"MSCI WORLD U$\",\n",
    "                \"INDIA\":\"MSCI WORLD U$\",\n",
    "                \"TAIWAN\":\"MSCI WORLD U$\",\n",
    "                \"JAPAN\":\"MSCI WORLD U$\",\n",
    "                \"SOUTH KOREA\":\"MSCI WORLD U$\",\n",
    "                \"SINGAPORE\":\"MSCI WORLD U$\",\n",
    "                \"MONGOLIA\":\"MSCI WORLD U$\",\n",
    "                \"CHINA\":\"MSCI WORLD U$\",\n",
    "                \"HONG KONG\":\"MSCI WORLD U$\",\n",
    "                \"INDONESIA\":\"MSCI WORLD U$\",\n",
    "                \"PHILIPPINES\":\"MSCI WORLD U$\",\n",
    "                \"BARBADOS\":\"MSCI WORLD U$\",\n",
    "                \"AUSTRALIA\":\"MSCI WORLD U$\",\n",
    "                \"NEW ZEALAND\":\"MSCI WORLD U$\",\n",
    "                \n",
    "                # missing values\n",
    "                np.nan:'NA'\n",
    "               \n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "data_abnormal_returns = data_returns[(data_returns[\"DATE\"] >= date_end)]\n",
    "data_abnormal_returns['NORMAL_RETURN'] = pd.NA\n",
    "\n",
    "for company_list in company_array:\n",
    "    company, cntry = company_list\n",
    "    \n",
    "    \n",
    "    print(company)\n",
    "    print(cntry)\n",
    "    \n",
    "    if (market_dict[cntry] != \"S&P 500 COMPOSITE\"):\n",
    "        print(f\"Skip {company} because no S&P\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    est_data_comp = data_returns[(data_returns['NAME']==company) & (data_returns[\"DATE\"] > date_start) & (data_returns[\"DATE\"] < date_end)]\n",
    "    est_data_ind = data_indices[(data_indices['NAME'] == market_dict[cntry]) & (data_indices['DATE'] > date_start) & (data_indices['DATE']< date_end)].reset_index(drop=True).set_index(est_data_comp.index)\n",
    "    est_data_ind = sm.add_constant(est_data_ind)\n",
    "    \n",
    "    if  est_data_comp[\"STOCK_PCT_RETURN\"].isna().sum()>0:\n",
    "        print(f\"Company: {company} skipped because NAs\")\n",
    "        continue \n",
    "    \n",
    "    reg = sm.OLS(est_data_comp['STOCK_PCT_RETURN'], est_data_ind[['const', \"INDEX_PCT_RETURN\"]]).fit(cov_type=\"HC3\")\n",
    "    \n",
    "    pred_data_ind = data_indices[(data_indices['NAME'] == market_dict[cntry]) & (data_indices['DATE'] >= date_end)].reset_index(drop=True)\n",
    "    pred_data_ind = sm.add_constant(pred_data_ind)\n",
    "    \n",
    "    \n",
    "    predicted = reg.predict(pred_data_ind[['const', 'INDEX_PCT_RETURN']])\n",
    "    predicted.name = 'NORMAL_RETURN'\n",
    "    predicted = predicted.set_axis(data_abnormal_returns.loc[data_abnormal_returns['NAME']== company, 'NORMAL_RETURN'].index)\n",
    "    \n",
    "    # print(reg.summary())\n",
    "    \n",
    "    \n",
    "    data_abnormal_returns.loc[data_abnormal_returns['NAME']== company, 'NORMAL_RETURN'] = predicted\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# NOTE NEED TO APPROACH THIS AGAIN - MAYBE USE STOCK PRICES AT THE DAILY LEVEL?      \n",
    "# MANY COMPANIES DON'T HAVE DATA THAT STARTS ENOUGH MONTHS BEFORE THE SHOCK SO MAYBE THE DAILY DATA WOULD BE BETTER \n",
    "# THEN I WOULD NEED TO CHOOSE A SPECIFIC DATE BETWEEN THE IPCC REPORT AND THE START OF THE CLIMATE WEEK\n",
    "#     \n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d81b70",
   "metadata": {},
   "source": [
    "#### Greenwashing Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60bb01ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR\n",
      "2022.0     2020\n",
      "2023.0     1859\n",
      "2021.0     1733\n",
      "2020.0     1355\n",
      "2019.0     1081\n",
      "2018.0      852\n",
      "2017.0      686\n",
      "2016.0      558\n",
      "2015.0      475\n",
      "2014.0      415\n",
      "2013.0      357\n",
      "2012.0      300\n",
      "2011.0      252\n",
      "2010.0      216\n",
      "2009.0      177\n",
      "2008.0      147\n",
      "2007.0      121\n",
      "2006.0       86\n",
      "2005.0       71\n",
      "2024.0       60\n",
      "2004.0       54\n",
      "2003.0       48\n",
      "2002.0       35\n",
      "2001.0       23\n",
      "2000.0       17\n",
      "1999.0        8\n",
      "1998.0        7\n",
      "20111.0       1\n",
      "201.0         1\n",
      "1989.0        1\n",
      "1997.0        1\n",
      "1996.0        1\n",
      "1995.0        1\n",
      "1994.0        1\n",
      "1993.0        1\n",
      "1992.0        1\n",
      "20167.0       1\n",
      "Name: count, dtype: int64\n",
      "ICB_INDUSTRY_NAME\n",
      "1    444\n",
      "5    441\n",
      "2    303\n",
      "3    241\n",
      "0    239\n",
      "4    204\n",
      "6      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# /////////////////////////////////////\n",
    "#       expanding\n",
    "# /////////////////////////////////////\n",
    "\n",
    "data_greenwashing['REPORT_LISTS'] = data_greenwashing['REPORT_LISTS'].apply(eval)\n",
    "\n",
    "# Expand each list entry into its own row\n",
    "data_greenwashing = data_greenwashing.explode('REPORT_LISTS', ignore_index=True)\n",
    "\n",
    "data_greenwashing['YEAR'] = data_greenwashing['REPORT_LISTS'].str.extract(r'(\\d+)')\n",
    "data_greenwashing['YEAR'] = data_greenwashing['YEAR'].astype(float)\n",
    "\n",
    "print(data_greenwashing.value_counts('YEAR'))\n",
    "\n",
    "obs_2014 = data_greenwashing[data_greenwashing['YEAR']==2014]\n",
    "\n",
    "# # ////////////////////////////////////////////////////////\n",
    "# # NOTE HOW MANY FIRMS WITH REPORTS IN THE YEARS I NEED\n",
    "g = data_greenwashing[(data_greenwashing[\"YEAR\"]>2016) & (data_greenwashing[\"YEAR\"]<2022)].groupby(\"NAME_SCRAPED\").count()\n",
    "print(g.value_counts(\"ICB_INDUSTRY_NAME\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da270d",
   "metadata": {},
   "source": [
    "Text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6993a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets that have a 2017 sustainability report\n",
    "\n",
    "greenwashing_2017 = data_greenwashing[data_greenwashing[\"YEAR\"]==2017]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ff887",
   "metadata": {},
   "source": [
    "Getting all 2017 sustainability reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ecfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "test = greenwashing_2017[\"NAME_SCRAPED\"]\n",
    "input_name = \"C:/Users/Jakub/OneDrive - Tilburg University/thesis data/responsibility reports\"\n",
    "\n",
    "filenames = ExtractFileName(series_names=test, input_dir=input_name)\n",
    "\n",
    "test_updated = test.str.lower().str.replace(\" \", \"_\").str.replace(\"?\", \"\").str.replace(\"|\", \"\")\n",
    "test_updated = test_updated.loc[~(test_updated.isin(filenames.keys()))]\n",
    "\n",
    "filenames_updated = ExtractNameYear(series_names=test_updated, input_dir=input_name, nlp_model=nlp)\n",
    "\n",
    "filenames.update(filenames_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0b94a",
   "metadata": {},
   "source": [
    "Dumping all of the text from PDF files into their respective TXT files for later processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f46f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtractAllText(filenames, \"2017\", \"../data/text_processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b916bb0",
   "metadata": {},
   "source": [
    "NLP model and greenwashing indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6c952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
